# -*- coding: utf-8 -*-
"""LLM Labeling "Trunk" - Top20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dfBKaxcPnYGvxm582QSEii43WG3OBUQd
"""

import pandas as pd
import openai
import json

def label_with_LLM(df, definitions, sentence_col = "sentences", model = "gpt-4o-mini"):
  results = []
  def_processed = "\n".join(f"{key}: {value}" for key, value in definitions.items())

  for idx, row in df.iterrows():
    sentence = row[sentence_col]

    prompt = (
        "You are a word definition classifier. Based on the sentences and available definitions, "
        "choose the best-fitting definition and assign a confidence score.\n\n"
        f"Sentence index: {idx}\n"
        f"Sentence: {sentence}\n\n"
        f"Definitions:\n{def_processed}\n\n"
        "Return output as JSON: {sentence, definition_key, definition, confidence}"
        "Return ONLY valid JSON. "
        "Do NOT wrap your answer in a code block. "
        "Do NOT include ```json or any backticks. "
        "Your response must start with '{' and end with '}'. "
        "Any extra text makes the answer invalid."
      )

    resp = openai.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
        )

    content = resp.choices[0].message.content.strip()
    print("RAW OUTPUT:\n", content)
    parsed = json.loads(content)
    results.append(parsed)

  return pd.DataFrame(results)

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/sentence_data_bert_dpgmm100k_wordsetlarge_mixedSentFalse_pca2d_priorNone.csv"
pre_df = pd.read_csv(path)

df = pre_df[["target", "sentences", "sense_label", "distance_to_sense_mean"]]
df_trunk = df[df["target"] == "trunk"].copy()
df_sorted = df_trunk.sort_values(by=["sense_label", "distance_to_sense_mean"])
top20_per_cluster = df_sorted.groupby("sense_label").head(20)

print(top20_per_cluster.shape)
print(top20_per_cluster['sense_label'].value_counts())

print(df_sorted["sense_label"].value_counts())

print(top20_per_cluster.head(10))

import getpass
openai.api_key = getpass.getpass("Enter API key:")

# The glosses come from elementary level of Wordsmyth kids
trunk_glosses = {
    "def_1": "A trunk is the main part of a tree.  The trunk grows from the tree's roots in the ground.  The branches of a tree grow out from its trunk.",
    "def_2": "Your trunk is the part of your body that your arms, legs, and neck are attached to.",
    "def_3": "A trunk is also a part of the body of an elephant.  An elephant's trunk forms its nose and part of its mouth. A trunk is long and curving, and the elephant can easily move it and bend it in different directions.  An elephant uses its trunk to breathe, to hold things, to feed itself, and many other things.",
    "def_4": "A trunk is a large, strong container that is used to store or carry things.  A trunk often has a lock.",
    "def_5": "A trunk is a large space in the back of a car that is used to carry or hold things such as bags, packages, and tools.",
    "def_6": "Trunks are a pair of short pants worn in sports such as track, swimming, and boxing."
}

LLM_label = label_with_LLM(top20_per_cluster, trunk_glosses)

LLM_label.to_csv("LLM_label_output.csv", index=False)

print(top20_per_cluster[["sentences", "sense_label"]].head(25))

# Manually checked, the sentences in the dataframe generated should have the same order
# as the original input dataframe, so every 20 sentences belong in the same cluster

LLM_label.columns

import matplotlib.pyplot as plt
import matplotlib.ticker as mtick

histogram_df = LLM_label.copy()

# Assign cluster IDs based on row order
cluster_size = 20
histogram_df["cluster_number"] = histogram_df.index // cluster_size + 1

# Plot one histogram per cluster
for num, group in histogram_df.groupby("cluster_number"):
    counts = group["definition_key"].value_counts()

    plt.figure(figsize=(6, 4))
    plt.bar_width = 0.3
    plt.ylim(0, 25)
    plt.gca().yaxis.set_major_locator(mtick.MaxNLocator(integer=True))
    plt.bar(counts.index, counts.values)
    for i, v in enumerate(counts.values):
      plt.text(i, v + 0.1, str(v), ha='center', va='bottom')
    plt.xlabel("Definition Key")
    plt.ylabel("Frequency")
    plt.title(f"Cluster {num} — Definition Frequency")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

import numpy as np
for num, group in histogram_df.groupby("cluster_number"):
    plt.figure(figsize=(6, 4))

    # histogram: bins from 0.0 to 1.0
    bins = np.linspace(0, 1, 11)  # 0.0, 0.1, 0.2, ..., 1.0
    plt.hist(group["confidence"], bins=bins, edgecolor="black", color="skyblue")

    plt.ylim(0, 20)

    plt.xlabel("Confidence Score")
    plt.ylabel("Frequency")
    plt.title(f"Cluster {num} — Confidence Histogram")

    plt.gca().yaxis.set_major_locator(mtick.MaxNLocator(integer=True))

    plt.tight_layout()
    plt.show()