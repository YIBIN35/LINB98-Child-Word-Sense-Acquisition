# -*- coding: utf-8 -*-
"""Trunk_Top_50_per_Cluster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LQgDipUFs-gw1smEbTiJXdclfNvvgQhS
"""

# Load the csv file directly in here
# Write a separate function for filtering out the sentences

import json
import numpy as np
import pandas as pd

from transformers import BertTokenizerFast, BertModel
import torch
import torch.nn.functional as F
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import SpectralCoclustering

import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/sentence_data_bert_dpgmm100k_wordsetlarge_mixedSentFalse_pca2d_priorNone.csv"

df = pd.read_csv(path, usecols = ["target", "sentences", "sense_label", "distance_to_sense_mean"], low_memory = False)

print(df.head())

print(df.shape)

# Recalculate the average:
# Compute embeddings for each one of them - calculate the mean
# Compute the distance of each embedding
# Make a new list for the new distance
# To test if the sentence correpsonds correctly - pick single datapoints & match the embeddings

# If use on dataframe: apply.() function (same as looping through list)

df_trunk = df[df["target"] == "trunk"].copy()

print(df_trunk.head())

print(df_trunk.shape)

print(df_trunk["sense_label"].unique())

# "closest-50-sentences" approach
df_sorted = df_trunk.sort_values(by=["sense_label", "distance_to_sense_mean"])

print(df_sorted.iloc[191:205])

print(df_sorted.iloc[0:20])

print(df_sorted.head(55))

unique_labels = df_sorted["sense_label"].unique()
print(unique_labels)

top50_per_cluster = df_sorted.groupby('sense_label').head(50)

print(top50_per_cluster.shape)

print(df_sorted['sense_label'].value_counts())

trunk_clusters = {str(label): [] for label in unique_labels}

print(trunk_clusters.keys())

for label in unique_labels:
  sentence_list = top50_per_cluster.loc[top50_per_cluster["sense_label"] == label, "sentences"].to_list()
  trunk_clusters[str(label)] = sentence_list

for key, lst in trunk_clusters.items():
    print(key, len(lst))

for label in trunk_clusters.keys():
  print(f"Label: {label}")
  print(trunk_clusters[label][:10])
  print()

# Example Sentences
# "well this is the trunk so the engine's in back so when when people when people are trying to fill a car they put all their stuff in the front ."
# "is that where the fuel is ? or is that the trunk ?"

# "trunk . trunk . trunk . trunk . trunk . trunk . knock_knock ."
# "trunk . trunk . trunk . trunk . trunk . shisha . shisha . shisha ."

# ". um , do you remember how an elephant can use its trunk like a , like a hand almost ?""
# "a baby elephant may drink its mother's milk until it is four or five years old . it curls back its trunk while feeding . that ."

# "you can't ... no . I wanna put it in the trunk . maybe I'll put it in the trunk ."
# "yeah . put em in the trunk ."

# Load the BERT model
# Load BERT model
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased", output_hidden_states=True)
model.eval()

! pip install ipdb

# Fixing the create_vector function
# Generate the BERT vectors
def bert_vectors(input):
    input_tokens = tokenizer(
        input, is_split_into_words=True, return_tensors="pt", truncation=True,
        max_length=512
    )
    hidden_vectors = model(
        **input_tokens, output_hidden_states=True
    ).hidden_states
    return hidden_vectors

# Modification of create_vectors()
def create_vectors_masked(sentence, target_word):
  # Masking is done for all sentences (cluster + glosses)

  # The processing for masking target words
  mask = "[MASK]"

  # In the original code, "target_word" was introduced in an earlier part of the child_clustering class,
  # here I assume it's basically indexing the position of the target word
  # target_index = sentence.index(target_word)

  # sentence_context = [word if word != target_word else mask for word in sentence] # This is splitting into characters one at a time

  sentence_split = sentence if isinstance(sentence, (list, tuple)) else str(sentence).split()
  for i in range(len(sentence_split)):
    if sentence_split[i] == target_word: # not assigning the element properly
      sentence_split[i] = mask

  # import ipdb; ipdb.set_trace() # check output

  # Averaging the last 6 layers of each vector & removing first and last token
  all_hidden_vectors = bert_vectors(sentence_split)
  last_n_layers_averaged = (torch.stack(all_hidden_vectors[-6:]).mean(dim=0).squeeze(0))
  last_n_layers_averaged = last_n_layers_averaged[1:-1]
  # assert len(tokenized_context) == len(last_n_layers_averaged)

  # context_vector branch - returning the vector instead of appending to the attribute in self
  context_averaged = last_n_layers_averaged.mean(dim=0).detach().numpy()
  return context_averaged

def create_vectors_unmasked(sentence, target_word):
  tokens = sentence if isinstance(sentence, (list, tuple)) else str(sentence).split()

  # Check sentence output

  all_hidden_vectors = bert_vectors(tokens)
  last_n_layers_averaged = (torch.stack(all_hidden_vectors[-6:]).mean(dim=0).squeeze(0))
  last_n_layers_averaged = last_n_layers_averaged[1:-1]
  # assert len(tokenized_context) == len(last_n_layers_averaged)

  context_averaged = last_n_layers_averaged.mean(dim=0).detach().numpy()
  return context_averaged

masked_cluster_average = []
unmasked_cluster_average = []

for label in trunk_clusters.keys():
  cluster_list = trunk_clusters[label]
  masked_list = []
  unmasked_list = []
  for sent in cluster_list:
    masked_output = create_vectors_masked(sent, "trunk")
    masked_list.append(masked_output)
    unmasked_output = create_vectors_unmasked(sent, "trunk")
    unmasked_list.append(unmasked_output)

  # take the average of all of the embeddings in that cluster
  masked_avg = np.vstack(masked_list).mean(axis=0)
  unmasked_avg = np.vstack(unmasked_list).mean(axis=0)

  # Append the average embeddings
  masked_cluster_average.append(masked_avg)
  unmasked_cluster_average.append(unmasked_avg)

# The glosses come from elementary level of Wordsmyth kids
trunk_glosses = {
    "def_1": "A trunk is the main part of a tree.  The trunk grows from the tree's roots in the ground.  The branches of a tree grow out from its trunk.",
    "def_2": "Your trunk is the part of your body that your arms, legs, and neck are attached to.",
    "def_3": "A trunk is also a part of the body of an elephant.  An elephant's trunk forms its nose and part of its mouth. A trunk is long and curving, and the elephant can easily move it and bend it in different directions.  An elephant uses its trunk to breathe, to hold things, to feed itself, and many other things.",
    "def_4": "A trunk is a large, strong container that is used to store or carry things.  A trunk often has a lock.",
    "def_5": "A trunk is a large space in the back of a car that is used to carry or hold things such as bags, packages, and tools.",
    "def_6": "Trunks are a pair of short pants worn in sports such as track, swimming, and boxing."
}

masked_glosses_average = []
unmasked_glosses_average = []

# Loop through the dictionary
for label in trunk_glosses.keys():
  gloss = trunk_glosses[label]
  masked_output = create_vectors_masked(gloss, "trunk")
  masked_glosses_average.append(masked_output)

  unmasked_output = create_vectors_unmasked(gloss, "trunk")
  unmasked_glosses_average.append(unmasked_output)

cosine_masked = cosine_similarity(masked_cluster_average, masked_glosses_average)
cosine_unmasked = cosine_similarity(unmasked_cluster_average, unmasked_glosses_average)

cluster_names = ["cluster_1", "cluster_2", "cluster_3", "cluster_4"]
gloss_names = ["def_1", "def_2", "def_3", "def_4", "def_5", "def_6"]

# Heatmap for masked
plt.figure(figsize=(10,6))

sns.heatmap(
    cosine_masked,
    xticklabels=gloss_names,   # x = clusters
    yticklabels=cluster_names,     # y = glosses
    cmap="coolwarm",
    vmin=0.5, vmax=1,
    annot=True, fmt=".2f",
    cbar_kws={"label": "Cosine similarity"}
)

plt.xlabel("Glosses")
plt.ylabel("Clusters")
plt.title("Cosine Similarity Heatmap (Masked Child) - top 50 closest to centriod per cluster")
plt.tight_layout()
plt.show()

# Heatmap for unmasked
plt.figure(figsize=(10,6))

sns.heatmap(
    cosine_unmasked,
    xticklabels=gloss_names,   # x = clusters
    yticklabels=cluster_names,     # y = glosses
    cmap="coolwarm",
    vmin=0.5, vmax=1,
    annot=True, fmt=".2f",
    cbar_kws={"label": "Cosine similarity"}
)

plt.xlabel("Glosses")
plt.ylabel("Clusters")
plt.title("Cosine Similarity Heatmap (Unmasked Child)- top 50 closest to centriod per cluster")
plt.tight_layout()
plt.show()