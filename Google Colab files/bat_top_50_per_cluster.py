# -*- coding: utf-8 -*-
"""Bat_Top_50_per_Cluster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZnludW2XPh2W_-nLfY5rkBw0DjJSlHa0
"""

# Load the csv file directly in here
# Write a separate function for filtering out the sentences

import json
import numpy as np
import pandas as pd

from transformers import BertTokenizerFast, BertModel
import torch
import torch.nn.functional as F
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import SpectralCoclustering

import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/sentence_data_bert_dpgmm100k_wordsetlarge_mixedSentFalse_pca2d_priorNone.csv"

pre_df = pd.read_csv(path)

print(pre_df.columns)

df = pre_df[["target", "sentences", "sense_label", "distance_to_sense_mean"]]

print(df.head())

print(df.shape)

df_bat = df[df["target"] == "bat"].copy()

print(df_bat['sense_label'].unique())

print(df_bat.head())

print(df_bat.shape)

# "closest-50-sentences" approach
df_sorted = df_bat.sort_values(by=["sense_label", "distance_to_sense_mean"])

print(df_sorted.shape)

print(df_sorted.iloc[90:110])

unique_labels = df_sorted["sense_label"].unique()
print(unique_labels)

top50_per_cluster = df_sorted.groupby("sense_label").head(50)

print(top50_per_cluster.shape)

print(df_sorted['sense_label'].value_counts())

print(df_sorted.iloc[0:20])

print(df_sorted.iloc[90:125])

bat_clusters = {str(label): [] for label in unique_labels}

print(bat_clusters.keys())

print(unique_labels)

for label in unique_labels:
  sentence_list = top50_per_cluster.loc[top50_per_cluster["sense_label"] == label, "sentences"].to_list()
  bat_clusters[str(label)] = sentence_list

for key, lst in bat_clusters.items():
    print(key, len(lst))

print(bat_clusters['0.0'][:5])

# print example sentences
for label in bat_clusters.keys():
  print(f"Label: {label}")
  print(bat_clusters[label][:5])
  print()

# Example sentences:
# "star . and I'm gonna play with the bat . no I'm gonna go put it over the tennis court okay find my bat ."
# 'what do you do with the bat ?'

# 'a apple flower a fork a clock . a saw a bat no /.'
# "yes , plastic . I'll never use a real bat for the rest of my life ."

# "it's baseball yeah . there's no ball though is there ? let me see . you're right it is a baseball bat ."
# "that's a baseball bat ."

# "a bat . one bat , two bats . what's that ?"
# "a baseball player and there's a bat . and that's a bat that you hit with and ..."

# Load the BERT model
# Load BERT model
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased", output_hidden_states=True)
model.eval()

! pip install ipdb

# Fixing the create_vector function
# Generate the BERT vectors
def bert_vectors(input):
    input_tokens = tokenizer(
        input, is_split_into_words=True, return_tensors="pt", truncation=True,
        max_length=512
    )
    hidden_vectors = model(
        **input_tokens, output_hidden_states=True
    ).hidden_states
    return hidden_vectors

# Modification of create_vectors()
def create_vectors_masked(sentence, target_word):
  # Masking is done for all sentence (cluster + glosses)

  # The processing for masking target words
  mask = "[MASK]"

  # In the original code, "target_word" was introduced in an earlier part of the child_clustering class,
  # here I assume it's basically indexing the position of the target word
  # target_index = sentence.index(target_word)

  # sentence_context = [word if word != target_word else mask for word in sentence] # This is splitting into characters one at a time

  sentence_split = sentence if isinstance(sentence, (list, tuple)) else str(sentence).split()
  for i in range(len(sentence_split)):
    if sentence_split[i] == target_word: # not assigning the element properly
      sentence_split[i] = mask

  import ipdb; ipdb.set_trace() # check output

  # Averaging the last 6 layers of each vector & removing first and last token
  all_hidden_vectors = bert_vectors(sentence_split)
  last_n_layers_averaged = (torch.stack(all_hidden_vectors[-6:]).mean(dim=0).squeeze(0))
  last_n_layers_averaged = last_n_layers_averaged[1:-1]
  # assert len(tokenized_context) == len(last_n_layers_averaged)

  # context_vector branch - returning the vector instead of appending to the attribute in self
  context_averaged = last_n_layers_averaged.mean(dim=0).detach().numpy()
  return context_averaged

def create_vectors_unmasked(sentence, target_word):
  tokens = sentence if isinstance(sentence, (list, tuple)) else str(sentence).split()

  # Check sentence output

  all_hidden_vectors = bert_vectors(tokens)
  last_n_layers_averaged = (torch.stack(all_hidden_vectors[-6:]).mean(dim=0).squeeze(0))
  last_n_layers_averaged = last_n_layers_averaged[1:-1]
  # assert len(tokenized_context) == len(last_n_layers_averaged)

  context_averaged = last_n_layers_averaged.mean(dim=0).detach().numpy()
  return context_averaged

masked_cluster_average = []
unmasked_cluster_average = []

for label in bat_clusters.keys():
  cluster_list = bat_clusters[label]
  masked_list = []
  unmasked_list = []
  for sent in cluster_list:
    masked_output = create_vectors_masked(sent, "bat")
    masked_list.append(masked_output)
    unmasked_output = create_vectors_unmasked(sent, "bat")
    unmasked_list.append(unmasked_output)

  # take the average of all of the embeddings in that cluster
  masked_avg = np.vstack(masked_list).mean(axis=0)
  unmasked_avg = np.vstack(unmasked_list).mean(axis=0)

  # Append the average embeddings
  masked_cluster_average.append(masked_avg)
  unmasked_cluster_average.append(unmasked_avg)

# The glosses come from elementary level of Wordsmyth kids
bat_glosses = {
    "def_1": "A bat is something we use to hit the ball in baseball or other sports. In baseball, the bat is made of wood or metal.",
    "def_2": "When you bat in the game of baseball, you have a chance to hit the ball.",
    "def_3": "	A bat is a small mammal that can fly. Bats have wings that are much longer than their bodies. They usually fly at night, and they eat many, many insects."
}

masked_glosses_average = []
unmasked_glosses_average = []

# Loop through the dictionary
for label in bat_glosses.keys():
  gloss = bat_glosses[label]
  masked_output = create_vectors_masked(gloss, "bat")
  masked_glosses_average.append(masked_output)

  unmasked_output = create_vectors_unmasked(gloss, "bat")
  unmasked_glosses_average.append(unmasked_output)

cosine_masked = cosine_similarity(masked_cluster_average, masked_glosses_average)
cosine_unmasked = cosine_similarity(unmasked_cluster_average, unmasked_glosses_average)

cluster_names = ["cluster_1", "cluster_2", "cluster_3", "cliuster_4"]
gloss_names = ["def_1", "def_2", "def_3"]

# Heatmap for masked
plt.figure(figsize=(10,6))

sns.heatmap(
    cosine_masked,
    xticklabels=gloss_names,   # x = clusters
    yticklabels=cluster_names,     # y = glosses
    cmap="coolwarm",
    vmin=0.5, vmax=1,
    annot=True, fmt=".2f",
    cbar_kws={"label": "Cosine similarity"}
)

plt.xlabel("Glosses")
plt.ylabel("Clusters")
plt.title("Cosine Similarity Heatmap (Masked Child) - top 50 closest to centriod per cluster")
plt.tight_layout()
plt.show()

# Heatmap for unmasked
plt.figure(figsize=(10,6))

sns.heatmap(
    cosine_unmasked,
    xticklabels=gloss_names,   # x = clusters
    yticklabels=cluster_names,     # y = glosses
    cmap="coolwarm",
    vmin=0.5, vmax=1,
    annot=True, fmt=".2f",
    cbar_kws={"label": "Cosine similarity"}
)

plt.xlabel("Glosses")
plt.ylabel("Clusters")
plt.title("Cosine Similarity Heatmap (Unmasked Child)- top 50 closest to centriod per cluster")
plt.tight_layout()
plt.show()