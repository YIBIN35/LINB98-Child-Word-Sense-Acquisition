# -*- coding: utf-8 -*-
"""PCA_Bat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pi1MAMCP6ZMA5XJmrfKEMgou_eocfVzv
"""

import json
import numpy as np
import pandas as pd

from transformers import BertTokenizerFast, BertModel
import torch
import torch.nn.functional as F
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import SpectralCoclustering
from sklearn.decomposition import PCA

import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/sentence_data_bert_dpgmm100k_wordsetlarge_mixedSentFalse_pca2d_priorNone.csv"

pre_df = pd.read_csv(path)
df = pre_df[["target", "sentences", "sense_label", "distance_to_sense_mean"]]
print(df.shape)

df_bat = df[df["target"] == "bat"].copy()
print(df_bat.shape)

# Compute the vectors of each sentence in each cluster
df_sorted = df_bat.sort_values(by=["sense_label"])
print(df_sorted['sense_label'].value_counts())

# Set the cluster labels
unique_labels = df_sorted['sense_label'].unique()
print(unique_labels)

print(df_sorted.head())

# Compute the first cluster
df_cluster1 = df_sorted[df_sorted['sense_label'] == unique_labels[0]].copy()
print(df_cluster1.shape)

cluster1_lst = []
for row in range(df_cluster1.shape[0]):
  cluster1_lst.append(df_cluster1.iloc[row]["sentences"])

print(len(cluster1_lst))

print(cluster1_lst[:5])

# Similarly, compute the other 3 clusters

# Second cluster
df_cluster2 = df_sorted[df_sorted['sense_label'] == unique_labels[1]].copy()
print(df_cluster2.shape)

cluster2_lst = []
for row in range(df_cluster2.shape[0]):
  cluster2_lst.append(df_cluster2.iloc[row]["sentences"])

# Third cluster
df_cluster3 = df_sorted[df_sorted['sense_label'] == unique_labels[2]].copy()
print(df_cluster3.shape)

cluster3_lst = []
for row in range(df_cluster3.shape[0]):
  cluster3_lst.append(df_cluster3.iloc[row]["sentences"])

# Fourth cluster
df_cluster4 = df_sorted[df_sorted['sense_label'] == unique_labels[3]].copy()
print(df_cluster4.shape)

cluster4_lst = []
for row in range(df_cluster4.shape[0]):
  cluster4_lst.append(df_cluster4.iloc[row]["sentences"])

# Record the length of each list
cluster1_len = len(cluster1_lst)
cluster2_len = len(cluster2_lst)
cluster3_len = len(cluster3_lst)
cluster4_len = len(cluster4_lst)

# Load the BERT model
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased", output_hidden_states=True)
model.eval()

mask = "[MASK]"

#Generate the BERT vectors
def bert_vectors(input):
    input_tokens = tokenizer(
        input, is_split_into_words=True, return_tensors="pt", truncation=True,
        max_length=512
    )
    hidden_vectors = model(
        **input_tokens, output_hidden_states=True
    ).hidden_states
    return hidden_vectors

# Create the embeddings (with MASKED)
def create_vectors_masked(sentence, target_word):
    sentence_split = sentence if isinstance(sentence, (list, tuple)) else str(sentence).split()
    for i in range(len(sentence_split)):
      if sentence_split[i] == target_word:
        sentence_split[i] = mask

    # import ipdb; ipdb.set_trace() # check output

    all_hidden_vectors = bert_vectors(sentence_split)
    last_n_layers_averaged = (torch.stack(all_hidden_vectors[-6:]).mean(dim=0).squeeze(0))
    last_n_layers_averaged = last_n_layers_averaged[1:-1]

    context_averaged = last_n_layers_averaged.mean(dim=0).detach().numpy()
    return context_averaged

# Get the embeddings for each sentence (masked version first) AND the centroid
cluster_embeddings = []
centroid_lst = []

# Dumb way without a loop
masked_1 = []
for sentence in cluster1_lst:
  vector = create_vectors_masked(sentence, "bat")
  cluster_embeddings.append(vector)
  masked_1.append(vector)
centroid_1 = np.vstack(masked_1).mean(axis=0)
centroid_lst.append(centroid_1)

masked_2 = []
for sentence in cluster2_lst:
  vector = create_vectors_masked(sentence, "bat")
  cluster_embeddings.append(vector)
  masked_2.append(vector)
centroid_2 = np.vstack(masked_2).mean(axis=0)
centroid_lst.append(centroid_2)

masked_3 = []
for sentence in cluster3_lst:
  vector = create_vectors_masked(sentence, "bat")
  cluster_embeddings.append(vector)
  masked_3.append(vector)
centroid_3 = np.vstack(masked_3).mean(axis=0)
centroid_lst.append(centroid_3)

masked_4 = []
for sentence in cluster4_lst:
  vector = create_vectors_masked(sentence, "bat")
  cluster_embeddings.append(vector)
  masked_4.append(vector)
centroid_4 = np.vstack(masked_4).mean(axis=0)
centroid_lst.append(centroid_4)

# print((masked_1 == cluster1_len) and (masked_2 == cluster2_len) and (masked_3 == cluster3_len) and (masked_4 == cluster4_len))

print(len(masked_1))
print(len(masked_2))
print(len(masked_3))
print(len(masked_4))

print(cluster1_len)
print(cluster2_len)
print(cluster3_len)
print(cluster4_len)

print(len(centroid_lst))

# Get the embeddings for the glosses
bat_glosses = {
    "def_1": "A bat is something we use to hit the ball in baseball or other sports. In baseball, the bat is made of wood or metal.",
    "def_2": "When you bat in the game of baseball, you have a chance to hit the ball.",
    "def_3": "	A bat is a small mammal that can fly. Bats have wings that are much longer than their bodies. They usually fly at night, and they eat many, many insects."
}

glosses_lst = []
for label in bat_glosses.keys():
  gloss = bat_glosses[label]
  masked_output = create_vectors_masked(gloss, "bat")
  glosses_lst.append(masked_output)

# Put everything together for PCA
input_lst = cluster_embeddings + centroid_lst + glosses_lst

# Compute PCA
pca = PCA(n_components=2, svd_solver='full', random_state=236)
pca_vectors = pca.fit_transform(input_lst)

# Compute the graphs
import matplotlib.pyplot as plt
import seaborn as sns

# Split the PCA list into different sections for color-coding
cluster_1 = pca_vectors[0:cluster1_len]
cluster_2 = pca_vectors[cluster1_len : cluster1_len+cluster2_len]
cluster_3 = pca_vectors[cluster1_len+cluster2_len : cluster1_len+cluster2_len+cluster3_len]
cluster_4 = pca_vectors[cluster1_len+cluster2_len+cluster3_len : cluster1_len+cluster2_len+cluster3_len+cluster4_len]

centroids = pca_vectors[cluster1_len+cluster2_len+cluster3_len+cluster4_len : cluster1_len+cluster2_len+cluster3_len+cluster4_len+len(centroid_lst)]

glosses = pca_vectors[-(len(glosses)):]

lst_groups = [cluster_1, cluster_2, cluster_3, cluster_4, centroids, glosses]
labels = ["cluster_1", "cluster_2", "cluster_3", "cluster_4", "centroids", "glosses"]


colors = [
    "tab:blue",
    "tab:purple",
    "tab:green",
    "tab:brown",
    "tab:red",
    "tab:orange",
]

plt.figure(figsize=(8, 6))

for group, label, color in zip(lst_groups, labels, colors):
    plt.scatter(
        group[:, 0],   # PC1
        group[:, 1],   # PC2
        label=label,
        color=color,
        alpha=0.9,
        # s=25
    )

plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA Scatterplot (Bat, masked)")
plt.legend()
plt.tight_layout()
plt.show()

# Try implementing the unmasked version

def create_vectors_unmasked(sentence, target_word):
  tokens = sentence if isinstance(sentence, (list, tuple)) else str(sentence).split()

  # Check sentence output

  all_hidden_vectors = bert_vectors(tokens)
  last_n_layers_averaged = (torch.stack(all_hidden_vectors[-6:]).mean(dim=0).squeeze(0))
  last_n_layers_averaged = last_n_layers_averaged[1:-1]
  # assert len(tokenized_context) == len(last_n_layers_averaged)

  context_averaged = last_n_layers_averaged.mean(dim=0).detach().numpy()
  return context_averaged

# Get the embeddings for each sentence (unmasked version) AND the centroid
cluster_embeddings_unmasked = []
centroid_lst_unmasked = []

# Dumb way without a loop
unmasked_1 = []
for sentence in cluster1_lst:
  vector = create_vectors_unmasked(sentence, "bat")
  cluster_embeddings_unmasked.append(vector)
  unmasked_1.append(vector)
centroid_1 = np.vstack(unmasked_1).mean(axis=0)
centroid_lst_unmasked.append(centroid_1)

unmasked_2 = []
for sentence in cluster2_lst:
  vector = create_vectors_unmasked(sentence, "bat")
  cluster_embeddings_unmasked.append(vector)
  unmasked_2.append(vector)
centroid_2 = np.vstack(unmasked_2).mean(axis=0)
centroid_lst_unmasked.append(centroid_2)

unmasked_3 = []
for sentence in cluster3_lst:
  vector = create_vectors_unmasked(sentence, "bat")
  cluster_embeddings_unmasked.append(vector)
  unmasked_3.append(vector)
centroid_3 = np.vstack(unmasked_3).mean(axis=0)
centroid_lst_unmasked.append(centroid_3)

unmasked_4 = []
for sentence in cluster4_lst:
  vector = create_vectors_unmasked(sentence, "bat")
  cluster_embeddings_unmasked.append(vector)
  unmasked_4.append(vector)
centroid_4 = np.vstack(unmasked_4).mean(axis=0)
centroid_lst_unmasked.append(centroid_4)

print((len(unmasked_1) == cluster1_len) and (len(unmasked_2) == cluster2_len) and (len(unmasked_3) == cluster3_len) and (len(unmasked_4) == cluster4_len))

glosses_lst_unmasked = []
for label in trunk_glosses.keys():
  gloss = trunk_glosses[label]
  unmasked_output = create_vectors_unmasked(gloss, "bat")
  glosses_lst_unmasked.append(unmasked_output)

print(len(centroid_lst_unmasked))

# Put everything together for PCA
input_lst_unmasked = cluster_embeddings_unmasked + centroid_lst_unmasked + glosses_lst_unmasked

# Compute PCA
pca = PCA(n_components=3, svd_solver='full', random_state=236)
pca_vectors_unmasked = pca.fit_transform(input_lst)

from mpl_toolkits.mplot3d import Axes3D

cluster_1 = pca_vectors_unmasked[0:cluster1_len]
cluster_2 = pca_vectors_unmasked[cluster1_len : cluster1_len+cluster2_len]
cluster_3 = pca_vectors_unmasked[cluster1_len+cluster2_len : cluster1_len+cluster2_len+cluster3_len]
cluster_4 = pca_vectors_unmasked[cluster1_len+cluster2_len+cluster3_len : cluster1_len+cluster2_len+cluster3_len+cluster4_len]

centroids = pca_vectors_unmasked[cluster1_len+cluster2_len+cluster3_len+cluster4_len : cluster1_len+cluster2_len+cluster3_len+cluster4_len+len(centroid_lst)]

glosses = pca_vectors_unmasked[-6:]

lst_groups = [cluster_1, cluster_2, cluster_3, cluster_4, centroids, glosses]
labels = ["cluster_1", "cluster_2", "cluster_3", "cluster_4", "centroids", "glosses"]


colors = [
    "tab:blue",
    "tab:purple",
    "tab:green",
    "tab:brown",
    "tab:red",
    "tab:orange",
]

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

for group, label, color in zip(lst_groups, labels, colors):
    group = np.array(group)
    ax.scatter(
        group[:, 0],
        group[:, 1],
        group[:, 2],
        label=label,
        color=color,
        s=15,
        alpha=0.7
    )
plt.title("PCA Scatterplot (Trunk, unmasked, 3D)")
ax.legend()
plt.show()